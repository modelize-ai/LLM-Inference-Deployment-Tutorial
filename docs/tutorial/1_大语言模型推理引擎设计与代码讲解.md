<h1 align="center">大语言模型推理引擎设计与代码讲解</h1>

# 概述

首先我们要明确的是，当谈论“大语言模型推理引擎”的时候，我们实际上谈的是 Server 端。现有一系列开源的知名推理框架，主要都将设计和优化的重点放在 Server 端上，这是因为大语言模型服务是一个计算密集型和访存密集型的服务，其性能瓶颈主要存在于涉及到运行大模型进行推理运算的服务端，相较之下客户端对系统端到端性能的影响则可以忽略不计。

> 在我们所提供的示例代码中包含了两种 Server 的实现，分别对应的代码模块为 `server/continuous_batching_server` 和 `server/static_batching_server`，我们将主要基于前者对推理引擎设计进行介绍和代码讲解。注意，虽然后者未完全实现这里介绍的所有部分，但这不意味着它们与之不相容。我们建议感兴趣的读者可以自行将缺失部分实现到后者中，以此来更公平地对比两种不同的 Server 之间的性能差异，并思考分析是否在所有的应用场景中 `CB Server` 性能都优于 `SB Server`。

在 NLP 1.0 时代，由于模型参数规模尚小，访存瓶颈的问题暂未显露，在这一时期的推理引擎主要侧重计算效率的优化，如 [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) 依托英伟达天然的优势，通过算子融合和 INT 8 数值量化的方式来减少计算操作，结合 `Static Batching` 机制最大化计算资源的利用率。

随着 GPT-3 的问世，NLP 进入 2.0 时代，Chat-GPT 之后，一大批开源大语言模型如雨后春笋般诞生，在业务系统中集成大语言模型的需求与日俱增。此时，推理引擎的性能瓶颈不再单单存在于计算（事实上，在现在的主流计算型显卡中，当 batch size 较小时，计算甚至已不再是瓶颈），访存效率对系统性能的影响和重要性上升到了第一位。同时，由于模型参数量急剧增加对 GPU 显存空间的大量挤占，和生成式语言模型提升解码效率而引入的 KV Cache 优化导致单卡剩余可用显存会随着生成序列长度线性减少，使得显存容量成为制约生成序列长度和下游应用场景的主要因素。在这样的背景下，一系列优化措施被相继提出，旨在提升访存效率和降低显存限制带来的影响：
- Flash Attention 和 Paged Attention 通过对 attention 的一系列优化来提高访存效率和/或计算效率；
- GPTQ 通过极低精度的量化(将 fp16 转换成 int4, int3 甚至是 int2 数值类型)来将模型对显存的占用降低 4 倍及以上。

> KV Cache (键值对缓存) 是一个双矩阵元组列表，每一个元组缓存着对应网络层历史的 key 和 value 矩阵，使用其能够减少生成式语言模型的计算量——即注意力模块在当前生成步只需计算最新 token 的 query、key 和 value，而无需再计算历史所有 tokens 的相关结果——从而达到加快解码过程（除第一步外）每步计算速度的目的。

此外，与 1.0 时代以 Bert 为代表的语言模型主要执行编码类任务，只需要一次前向计算即可获取最终预测结果不同；2.0 时代，GPT 为代表的语言模型执行的往往是解码类（生成式）任务，模型需要多次执行前向计算以生成文本序列。这意味着 1.0 时代以 Static Batching 为代表的早期动态批处理策略随着主要任务的变换而不再适用，由此 Hugging Face 提出了 Continuous Batching 策略，并针对基于大模型的 SaaS 服务接收的请求在配置的生成策略上往往不相同的流量特征，进一步设计了异构解码策略的解决方案。

> 关于大语言模型背景下访存成为系统性能瓶颈的原因，简单来说是因为模型参数量变大导致从 GPU 的高带宽内存中读写一次网络权重的开销（时间）超过了使用该网络进行一次计算的时间。

# 请求打包模块

### Static Batching

Static Batching (静态批处理) 是一种请求打包策略，其通过将一个时间窗口内在不同时刻到达的请求打包在一起后，一次性给到工作者（语言模型）处理，来提高算力利用率，从而增大系统吞吐。这主要得益于 GPU 强大的矩阵并行计算能力，使得一次执行多个矩阵乘法的时间（远）少于多个矩阵顺次执行的时间之和。

但是这一策略存在以下两个问题：
- 额外增加的固定等待时间：为了能够尽可能多地将请求打包以进行批处理，往往会设置一个等待时间来累积请求，在流量较小的情况下这意味着累积一定量的请求需要设置较大的等待时间，反而加大了响应延迟。
- 所有请求必须一同被返回：解包动作在模型完成端到端推理后执行，这在用以 Bert 为代表的语言模型执行编码类任务时不是问题，因为此时只会执行一次前向计算；但对于解码类任务，完成文本序列的生成需要执行多步前向计算，而不同请求之间生成的序列长度往往不同，导致所有请求都必须等到请求包中生成序列最大的请求被执行完成后才能被返回。

我们在 `server/static_batching_server` 的 [`batcher.py`](https://github.com/modelize-ai/LLM-Inference-Deployment-Tutorial/blob/24ad630cc8719f6c833fcae20735387ceff8c5e2/code/server/static_batching_server/batcher.py#L41) 中实现了简单的 Static Batching 逻辑：

```python
...

class Package(BaseModel):
    ids: List[UUID] = Field(default=...)
    prompts: List[str] = Field(default=...)
    generation_config: HuggingFaceGenerationConfig = Field(default=...)

    def __hash__(self):
        return hash(self.generation_config)

    @property
    def workload(self):
        return len(self.prompts) * self.generation_config.num_beams

    def add(self, prompt: str, uid: UUID):
        self.prompts.append(prompt)
        self.ids.append(uid)

    def __repr__(self):
        return f"Package(workload={self.workload})"

    def __str__(self):
        return self.__repr__()


class Batcher:
    def __init__(self, config: BatcherConfig, logger: Optional[Logger] = None):
        self.config = config
        self.logger = logger if logger else getLogger(__name__)

        self.inputs: List[Tuple[HuggingFaceCompletionInputs, UUID]] = []  # 请求等待队列

    def pack(self) -> Optional[Package]:
        if not self.inputs:
            return None

        # =============================
        # 策略 1:
        #   将等待队列中的第一个请求添加到请求包中，并从等待队列中剩余的请求里取出生成策略配置参数相同的其他任务一并添加
        #   到请求包，循环往复直到请求包达到容量上限或等待队列中没有其他“同质化”请求。
        # =============================

        inp, inp_id = self.inputs.pop(0)
        package = Package(ids=[inp_id], prompts=[inp.prompt], generation_config=inp.generation_config)
        inputs = []
        while self.inputs:
            if package.workload > self.config.package_max_workload:  # package is full, put back and return
                self.inputs = inputs + self.inputs
                self.logger.debug(msg=str(package))
                return package
            inp, inp_id = self.inputs.pop(0)
            if hash(inp.generation_config) != hash(package):  # gen_config is different, put back
                inputs.append((inp, inp_id))
            else:
                package.add(inp.prompt, inp_id)
        return package
    
        # 源代码中还给出了另一种策略的实现，感兴趣的读者可以跳转至源码文件阅读

    def add(self, inp: HuggingFaceCompletionInputs, inp_id: UUID):
        self.inputs.append((inp, inp_id))

...
```
并在 `server/static_batching_server` 的 [`server.py`](https://github.com/modelize-ai/LLM-Inference-Deployment-Tutorial/blob/24ad630cc8719f6c833fcae20735387ceff8c5e2/code/server/static_batching_server/server.py#L54) 中利用一个线程周期性地调用 `Batcher.pack` 方法来不断地打包源源不断的请求：

```python
...

class Server:
    def __init__(self, config: ServerConfig, logger: Optional[Logger] = None):
        ...

        Thread(target=self._run, daemon=True).start()
        
        ...

    def _run(self):
        while True:
            package = self.batcher.pack()
            if package is not None:
                self.outputs.update(self.worker.execute(package.prompts, package.ids, package.generation_config))
            else:
                time.sleep(self.batcher_config.packaging_interval_seconds)

...
```

> 在我们提供的实现中，只有“同质化”的请求能够被添加到同一个请求包中，这是因为在 `server/static_batching_server` 的 `worker.py` 中我们直接使用了 Hugging Face 的 `model.generate` 接口，但这无疑是十分低效的。感兴趣的读者可以在学习完[生成工具模块](#生成工具模块)一节后，自己动手尝试实现异构解码策略。

### Continuous Batching

Continuous Batching (持续批处理，也称动态批处理) 是一种针对生成式语言模型而新提出的更灵活的请求打包策略，其通过独立地看待每一步前向计算，来解决 Static Batching 的两大缺陷，从而实现：
- 后到达的请求能够随时插入请求包：在每一步前向计算开始前，执行一次请求包的整理动作，在未达到资源限制的前提下将新到达的请求加入到请求包中。
- 先完成的请求能够随时移出请求包：在每一步前向计算完成后，执行一次请求包的清理动作，将被标记为结束的请求从请求包中移除并返回。

同时，若对外提供 SaaS 服务，不同的请求往往带有不同的生成策略超参，因此实现 Continuous Batching 的推理引擎还需要同时实现异构解码策略，我们将在[生成工具模块](#生成工具模块)一节展开说明。

但是，Continuous Batching 的策略并非完美无暇，这是因为，语言模型生成文本的过程可分为两阶段操作：prefill (预填充)和 decode (解码)。prefill 只需执行一次，发生在模型对新到达的请求进行的第一次计算，此时需要对整个 prompt 构建 KV Cache 并预测下一个 token；往后的每一步计算皆为 decode，这时模型只需要对上一步得到的 token 进行计算和预测再下一个 token，并将 key 和 value 拼接到 KV Cache 的对应矩阵，而无需对历史的 tokens 重新计算 keys 和 values。显然，prefill 操作与 decode 操作所做的内容以及计算量（尤其当 prompt 特别长的时候）是不同的，这意味着在每一步的执行，若请求包中插入了新到达的请求，模型必须首先对新请求额外执行 prefill 操作后，再对所有请求一同执行 decode 操作，这无疑对先前到达的请求不公平，因为它们必须等待新的请求完成“初始化”。所幸的是，每个请求的 prefill 操作只需执行一次，与执行一系列 decode 操作所需的总时间相比，这额外的开销便显得微不足道了。

我们在 `server/continuous_batching_server` 的 [`batcher.py`](https://github.com/modelize-ai/LLM-Inference-Deployment-Tutorial/blob/24ad630cc8719f6c833fcae20735387ceff8c5e2/code/server/continuous_batching_server/batcher.py#L85) 中实现了 Continuous Batching 的逻辑，共分为三大步骤：

> 我们在 `Batcher` 类中定义了三个队列，它们分别是：
> - 等待队列 `self.waiting`: 未被加入到请求包中的请求都被存放于该队列
> - 执行队列 `self.running`: 被加入到请求包中进行处理的请求被存放于该队列
> - 交换队列 `self.preempting`: 当剩余的 GPU 显存不足以请求包中的所有请求继续生成新的 token 时，将请求包中到达时间较晚的请求的 KV Cache 依次转移到 CPU 内存中直至剩余请求能够被执行，被转移的请求被存放于该队列

首先，从执行队列中移除在上一步模型执行后完成文本序列生成的请求:

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        self._free_finished_beams()
        running = []
        finishing = []
        while self.running:
            beam_group = self.running.pop(0)
            if beam_group.is_finished:
                finishing.append(beam_group)
            else:
                running.append(beam_group)
        self.running = running
        
        ...
    
        return batch, blocks_to_copy, blocks_to_swap_in, blocks_to_swap_out, finishing
```

然后，对运行队列中仍需继续进行文本序列生成的请求分配显存空间，这一步骤进一步被分成四个阶段执行：

1. 统计运行队列中所有请求所需增量分配的 GPU 显存空间的总块数

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        ...

        running = []
        swapping_out = []
        swapping_in = []
        blocks_to_copy: Dict[int, List[int]] = defaultdict(list)  # 需要拷贝到派生出的子 beam 上的内存块
        blocks_to_swap_in: Dict[int, int] = {}  # 需要被交换回 GPU 的内存块
        blocks_to_swap_out: Dict[int, int] = {}  # 需要被交换至 CPU 的内存块
        run_request2num_append_blocks = defaultdict(int)
        
        for beam_group in self.running:
            run_request2num_append_blocks[beam_group.request_id] = 0
            for beam in beam_group.get_beams(BeamStatus.RUNNING):
                if self.cache_manager.is_need_to_append_slot(beam):
                    run_request2num_append_blocks[beam_group.request_id] += 1
        
        ...
```

2. 如果空闲的 GPU 显存空间总块数少于增量分配所需的总块数，则从运行队列尾端起依次将每个请求所占用的 GPU 显存空间交互至 CPU，直至剩余空间足够用于分配给运行队列前端的其他请求

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        ...

        running = []
        swapping_out = []
        swapping_in = []
        blocks_to_copy: Dict[int, List[int]] = defaultdict(list)  # 需要拷贝到派生出的子 beam 上的内存块
        blocks_to_swap_in: Dict[int, int] = {}  # 需要被交换回 GPU 的内存块
        blocks_to_swap_out: Dict[int, int] = {}  # 需要被交换至 CPU 的内存块
        run_request2num_append_blocks = defaultdict(int)

        ...

        while self.cache_manager.allocator.num_free_blocks < sum(run_request2num_append_blocks.values()):
            beam_group = self.running.pop(-1)
            num_append_blocks = run_request2num_append_blocks.pop(beam_group.request_id)
            if num_append_blocks == 0:
                running.insert(0, beam_group)
                continue
            if not self.cache_manager.can_swap_out(beam_group):
                # 我们在这里通过报错来简单处理，但这在生产上并不友好，vLLM 中实现了中断请求的机制，其中一个触发条件便是在这里，
                # 此时会将循环到的请求设置为中断执行状态 (beam_group) 并放入结束列表中直接返回
                raise RuntimeError("No enough CPU RAM to swap out")
            else:
                blocks_to_swap_out.update(self.cache_manager.swap_out(beam_group))
                for beam in beam_group.get_beams(BeamStatus.RUNNING):
                    beam_group.update_beam_status(beam, BeamStatus.SWAPPED)
                swapping_out.insert(0, beam_group)
        self.running += running
        self.preempting += swapping_out

        ...
```

3. 如果不发生 GPU->CPU 交互且存在被交换至 CPU 的等待继续被处理的请求时，将这些请求按优先级依次交换回 GPU 直至无法被换回

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        ...

        running = []
        swapping_out = []
        swapping_in = []
        blocks_to_copy: Dict[int, List[int]] = defaultdict(list)  # 需要拷贝到派生出的子 beam 上的内存块
        blocks_to_swap_in: Dict[int, int] = {}  # 需要被交换回 GPU 的内存块
        blocks_to_swap_out: Dict[int, int] = {}  # 需要被交换至 CPU 的内存块
        run_request2num_append_blocks = defaultdict(int)

        ...

        if not swapping_out:
            preserved_num_blocks = sum(run_request2num_append_blocks.values())
            while self.preempting:
                beam_group = self.preempting[0]
                if not self.cache_manager.can_swap_in(beam_group, preserved_num_blocks):
                    if not self.running:
                        raise RuntimeError(
                            "running queue is empty but still can't swap in request, "
                            "please consider increase num_blocks or decrease max tokens number"
                        )
                    else:
                        break  # exceed num available free gpu blocks if swap in this beam_group, break
                beam_group = self.preempting.pop(0)
                blocks_to_swap_in.update(self.cache_manager.swap_in(beam_group))
                for beam in beam_group.get_beams(BeamStatus.SWAPPED):
                    beam_group.update_beam_status(beam, BeamStatus.RUNNING)
                swapping_in.append(beam_group)
                preserved_num_blocks += sum(
                    [
                        self.cache_manager.is_need_to_append_slot(beam)
                        for beam in beam_group.get_beams(BeamStatus.RUNNING)
                    ]
                )
            self.running += swapping_in

        ...
```

4. 为运行队列中剩余的请求进行缓存空间的分配

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        ...

        running = []
        swapping_out = []
        swapping_in = []
        blocks_to_copy: Dict[int, List[int]] = defaultdict(list)  # 需要拷贝到派生出的子 beam 上的内存块
        blocks_to_swap_in: Dict[int, int] = {}  # 需要被交换回 GPU 的内存块
        blocks_to_swap_out: Dict[int, int] = {}  # 需要被交换至 CPU 的内存块
        run_request2num_append_blocks = defaultdict(int)

        ...
        
        for beam_group in self.running:
            beams = beam_group.get_beams(BeamStatus.RUNNING)
            for beam in beams:
                self._append_slot(beam, blocks_to_copy)
                block_ids = self.cache_manager.get_block_table(beam.beam_id)
                batch.add(beam, block_ids)
                beam_group.beams.pop(beam.beam_id)
        
        ...
```

最后，在不发生 GPU->CPU 交互的情况下，尝试将等待队列中的请求移至运行队列：

```python
class Batcher:
    ...
    
    def schedule(self) -> Tuple[
        Batch,
        Dict[int, List[int]],
        Dict[int, int],
        Dict[int, int],
        List[BeamGroup]
    ]:
        batch = Batch()

        ...

        running = []
        swapping_out = []
        swapping_in = []
        blocks_to_copy: Dict[int, List[int]] = defaultdict(list)  # 需要拷贝到派生出的子 beam 上的内存块
        blocks_to_swap_in: Dict[int, int] = {}  # 需要被交换回 GPU 的内存块
        blocks_to_swap_out: Dict[int, int] = {}  # 需要被交换至 CPU 的内存块
        run_request2num_append_blocks = defaultdict(int)

        ...

        batch_tokens = batch.num_beams
        if (not swapping_out or not self.running) and not self.preempting:
            while self.waiting:
                beam_group = self.waiting[0]
                beam = beam_group.get_beams()[0]
                if batch_tokens + beam.num_tokens > self.batcher_config.batch_max_tokens:
                    break
                if batch.num_beams + 1 > self.batcher_config.batch_max_beams:
                    break
                has_cache_space = self._allocate(beam)
                if not has_cache_space:
                    break
                beam_group = self.waiting.pop(0)
                beam = beam_group.get_beams()[0]
                batch_tokens += beam.num_tokens
                beam_group.update_beam_status(beam, BeamStatus.RUNNING)
                self.running.append(beam_group)
                block_ids = self.cache_manager.get_block_table(beam.beam_id)
                batch.add(beam, block_ids)
                beam_group.beams.pop(beam.beam_id)

        return batch, blocks_to_copy, blocks_to_swap_in, blocks_to_swap_out, finishing
```

我们在 `Batcher.schedule` 方法中涉及到了大量有关内存状态查询的操作，并利用字典来记录需要执行内存拷贝和交换操作的内存块，在下一小节，我们将进一步了解相关操作的具体逻辑实现。


# 缓存管理模块




# 模型执行模块



# 模型工具模块

### Flash Attention


### Paged Attention


### GPTQ



# 生成工具模块



# Server 封装和 HTTP 服务化
